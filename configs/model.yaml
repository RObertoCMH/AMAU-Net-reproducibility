# configs/model.yaml
# -------------------------------------------------------------------
# AMAU-Net â€” MODEL CONFIG 
#
# We use a single implementation class: UNetWithAttention.
# - Plain U-Net: attention disabled.
# - U-Net + attributes: 4-channel input, attention disabled.
# - AMAU-Net: 4-channel input + selected CBAM insertions + bottleneck self-attention.
#
# CBAM insertion points implemented in code (layer IDs):
#   encoder: 2, 3, 4  (after conv2/conv3/conv4)
#   decoder: 6, 7, 8  (after conv6/conv7/conv8)
# Paper setting enables CBAM at [2, 4, 8] and self-attention at the bottleneck.
# -------------------------------------------------------------------

implementation:
  class_name: "UNetWithAttention"

defaults:
  n_classes: 6
  n_filters: 80
  dropout: 0.5
  batchnorm: true

cbam:
  supported_layer_ids: [2, 3, 4, 6, 7, 8]

variants:
  unet_amp:
    description: "Amplitude-only baseline (plain U-Net)."
    in_channels: 1
    use_self_attention: false
    cbam_in_layers: []

  unet_attrs:
    description: "Multi-attribute U-Net (no attention)."
    in_channels: 4
    use_self_attention: false
    cbam_in_layers: []

  amau_net:
    description: "AMAU-Net (attributes + selected CBAM + bottleneck self-attention)."
    in_channels: 4
    use_self_attention: true
    cbam_in_layers: [2, 4, 8]

