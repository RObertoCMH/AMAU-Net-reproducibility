# configs/model.yaml
# -------------------------------------------------------------------
# AMAU-Net â€” MODEL CONFIG (matches UNetWithAttention implementation)
#
# The code implements a U-Net backbone with optional attention:
# - CBAM blocks can be enabled only at specific layer IDs implemented in code:
#   encoder: 2, 3, 4   (after conv2/conv3/conv4)
#   decoder: 6, 7, 8   (after conv6/conv7/conv8)
# - A single self-attention block can be enabled at the bottleneck (after conv5).
#
# For this manuscript, AMAU-Net is modular in design, but we enable CBAM only at
# three locations (2 encoder + 1 decoder) plus bottleneck self-attention to keep
# overhead small and ensure a fair ablation.
# -------------------------------------------------------------------

model:
  class_name: "UNetWithAttention"
  n_classes: 6
  in_channels: 4          # 1 for amplitude-only baseline; 4 for multi-attribute input
  n_filters: 80           # matches n_filters default in code
  dropout: 0.5            # base dropout value passed to Dropout2d
  batchnorm: true

attention:
  use_self_attention: true   # enables SelfAttentionBlock at bottleneck (after conv5)

  cbam:
    # CBAM layer IDs supported by the current implementation
    supported_layer_ids: [2, 3, 4, 6, 7, 8]

    # CBAM layer IDs actually used in this work (paper setting)
    # Choose the three IDs that match your experiment (2 encoder + 1 decoder).
    cbam_in_layers: [2, 3, 6]
